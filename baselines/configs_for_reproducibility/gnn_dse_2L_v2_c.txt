model                      : our
v2_db                      : True
use_redis                  : False
task                       : class
subtask                    : train
sequence_modeling          : False
multi_modality             : False
load_db                    : None
load_model                 : None
dataset                    : programl
benchmarks                 : ['machsuite',
                           :  'poly']
tag                        : wmp-d
round_num                  : 3
graph_type                 : 
graph_transformer_option   : None
gae_T                      : False
gae_P                      : False
SSL                        : False
load_pretrained_GNN        : False
class_model_path           : None
all_kernels                : True
sample_finetune            : False
FT_extra                   : False
only_common_db             : False
test_extra                 : False
only_new_points            : False
new_speedup                : True
feature_extract            : False
ignore_kernels             : []
test_kernels               : ['gemm-p-large',
                           :  'covariance',
                           :  'gemm-ncubed',
                           :  'symm',
                           :  'fdtd-2d-large',
                           :  'trmm-opt']
val_ratio_in_test_kernels  : 0
val_ratio_in_train_kernels : 0.15
test_ratio_in_train_kernels : 0.15
tvt_split_by               : kernels_inductive
itype_mask_perc            : 0
gtype                      : programl
only_pragma_nodes          : False
encode_full_text           : None
fulltext_dim               : None
MAML                       : False
force_regen                : False
load_encoders_label        : None
encoder_path               : None
model_tag                  : test
activation                 : elu
outlier_removal            : None
no_pragma                  : False
num_layers                 : 2
D                          : 512
target                     : ['perf',
                           :  'util-LUT',
                           :  'util-FF',
                           :  'util-DSP',
                           :  'util-BRAM']
gnn_type                   : transformer
min_allowed_latency        : 100.0
encode_edge                : True
encode_edge_position       : False
ptrans                     : False
jkn_mode                   : max
jkn_enable                 : True
node_attention             : True
node_attention_MLP         : False
separate_P_T               : False
pragma_encoder             : True
pragma_uniform_encoder     : True
epsilon                    : 0.001
normalizer                 : 10000000.0
util_normalizer            : 1
max_number                 : 10000000000.0
norm_method                : speedup-log2
target_preproc             : None
target_convert_back        : True
invalid                    : False
multi_target               : True
activation_type            : elu
margin_loss                : False
save_model                 : True
save_every_epoch           : 10000
encode_log                 : False
baseline_no_graph          : False
target_factor              : 1
target_transform           : None
loss_scale                 : {'perf': 1.0,
                           :  'util-DSP': 1.0,
                           :  'util-BRAM': 1.0,
                           :  'util-LUT': 1.0,
                           :  'util-FF': 1.0}
pairwise_class             : False
batch_size                 : 64
data_loader_num_workers    : 0
device                     : cuda:3
epoch_num                  : 200
debug_iter                 : -1
ignore_testing             : True
ignore_validation          : False
shuffle                    : True
opt_type                   : AdamW
lr                         : 0.0001
max_grad_norm              : None
weight_decay               : 0.01
plot_pred_points           : True
fix_randomness             : True
random_seed                : 123
user                       : xxx
hostname                   : xxxx
exp_name                   : 
ts                         : 2023-05-30T22-45-11.537613

Net(
  (conv_first): TransformerConv(150, 512, heads=1)
  (conv_layers): ModuleList(
    (0): TransformerConv(512, 512, heads=1)
  )
  (jkn): JumpingKnowledge(max)
  (gate_nn): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
  (glob): MyGlobalAttention(gate_nn=Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  ), nn=None)
  (loss_function): CrossEntropyLoss()
  (MLPs): ModuleDict(
    (perf): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=128, bias=True)
        (2): Linear(in_features=128, out_features=64, bias=True)
        (3): Linear(in_features=64, out_features=32, bias=True)
        (4): Linear(in_features=32, out_features=16, bias=True)
        (5): Linear(in_features=16, out_features=2, bias=True)
      )
    )
  )
)
